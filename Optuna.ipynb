{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d0b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "data = pd.read_excel(r'C:\\Users\\Radhika K J\\Downloads\\Project6\\Project Group 6\\ECommerceDataset.xlsx',sheet_name='E Comm')\n",
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ffcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying a small chunk of data.\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1976f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the same values with different respresentations into one single value\n",
    "data['PreferredLoginDevice'] = data['PreferredLoginDevice'].replace('Phone', 'Mobile Phone')\n",
    "data['PreferredPaymentMode'] = data['PreferredPaymentMode'].replace('CC', 'Credit Card').replace('COD', 'Cash on Delivery')\n",
    "data['PreferedOrderCat'] = data['PreferedOrderCat'].replace('Mobile', 'Mobile Phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda93883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the OrderCount because orders have to be whole numbers.\n",
    "data['OrderCount'] = round(data['OrderCount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c548d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of columns.\n",
    "data['CityTier'] = data['CityTier'].astype('object')\n",
    "data['SatisfactionScore'] = data['SatisfactionScore'].astype('object')\n",
    "data['Complain'] = data['Complain'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the missing values using KNN.\n",
    "missing_cols = [col for col in data.columns if data[col].isnull().any()]\n",
    "from sklearn.impute import KNNImputer\n",
    "impute_knn = KNNImputer(n_neighbors=5)\n",
    "data_missing = data[missing_cols]\n",
    "imputed_data = impute_knn.fit_transform(data_missing)\n",
    "data_imputed = pd.concat([data.drop(missing_cols, axis=1), pd.DataFrame(imputed_data, columns=missing_cols)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e95be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize the values above 99th percentile to 99th percentile\n",
    "from scipy.stats.mstats import winsorize\n",
    "winsorize(data_imputed['Tenure'], limits=(0, 0.01), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab450016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Winsorize the values above 95th percentile to 95th percentile\n",
    "winsorize(data_imputed['NumberOfAddress'], limits=(0, 0.05), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934c4a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting Two Sample T-Test to see which numerical columns to select\n",
    "from scipy.stats import ttest_ind\n",
    "def num_stats(num_col):\n",
    "    group_0 = data_imputed[data_imputed['Churn']==0][num_col]\n",
    "    group_1 = data_imputed[data_imputed['Churn']==1][num_col]\n",
    "\n",
    "    t_stat, p_value = ttest_ind(group_0,group_1,equal_var=False)\n",
    "\n",
    "    print('P-value : ', p_value)\n",
    "    if(p_value<0.05):\n",
    "        print('Reject null hypothesis')\n",
    "    else:\n",
    "        print('Do not reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfbc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting chi-square test of independence on categorical columns.\n",
    "from scipy.stats import chi2_contingency\n",
    "def chisq_test(cat_col, df):\n",
    "    CrossTabResult=pd.crosstab(index=df['Churn'], columns=df[cat_col])\n",
    "    ChiSqResult = chi2_contingency(CrossTabResult)\n",
    "        \n",
    "    if (ChiSqResult[1] < 0.05):\n",
    "        print('P-Value :', ChiSqResult[1])\n",
    "        print('Reject null hypothesis')\n",
    "    else:\n",
    "        print('P-Value :', ChiSqResult[1])\n",
    "        print('Do not reject null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d3c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = data_imputed.select_dtypes(include=['int', 'float']).columns.tolist()\n",
    "numerical_cols.remove('Churn')\n",
    "categorical_cols = data_imputed.select_dtypes(include=['object']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb60a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    print('Column Name : ', col)\n",
    "    num_stats(col)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b30850a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    print('Column Name : ', col)\n",
    "    chisq_test(col, data_imputed)\n",
    "    print('---------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ce7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns where we accept the null hypothesis of the column does not affect the target 'Churn'\n",
    "data_imputed.drop(columns=['CustomerID','HourSpendOnApp','OrderCount','OrderAmountHikeFromlastYear','CouponUsed'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979ad878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using one hot encoding for the categorical columns.\n",
    "data_imputed[categorical_cols] = data_imputed[categorical_cols].astype('category')\n",
    "df_encoded = pd.get_dummies(data_imputed,columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cb791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is imbalanced. So we use an oversampling method of SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=33)\n",
    "X = df_encoded.drop('Churn', axis=1)\n",
    "y = df_encoded['Churn']\n",
    "x_smote, y_smote = smote.fit_resample(X, y)\n",
    "df_encoded = pd.DataFrame(x_smote, columns=df_encoded.drop('Churn', axis=1).columns)\n",
    "df_encoded['Churn'] = y_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf72f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X = df_encoded.drop('Churn', axis=1)\n",
    "cols = X.columns\n",
    "y= df_encoded.Churn\n",
    "# Using StandarScaler to scale the values\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X,columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa229fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state= 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0075e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 0.01, 1.0)\n",
    "    algorithm = trial.suggest_categorical(\"algorithm\", [\"SAMME\"])\n",
    "    base_estimator = trial.suggest_categorical(\"base_estimator\", [\"decision_tree\", \"logistic_regression\", \"svm\"])\n",
    "\n",
    "    if base_estimator == \"decision_tree\":\n",
    "        criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n",
    "        max_depth = trial.suggest_int(\"max_depth\", 2, 32)\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 10)\n",
    "\n",
    "        base_estimator = DecisionTreeClassifier(\n",
    "            criterion=criterion,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split\n",
    "        )\n",
    "    elif base_estimator == \"logistic_regression\":\n",
    "        C = trial.suggest_float(\"C\", 0.1, 10.0)\n",
    "        penalty = trial.suggest_categorical(\"penalty\", [\"l2\",None])\n",
    "        solver = trial.suggest_categorical(\"solver\", [\"lbfgs\"])\n",
    "\n",
    "        base_estimator = LogisticRegression(\n",
    "            C=C,\n",
    "            penalty=penalty,\n",
    "            solver=solver\n",
    "        )\n",
    "    elif base_estimator == \"svm\":\n",
    "        C = trial.suggest_float(\"C\", 0.1, 10.0)\n",
    "        kernel = trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\", \"sigmoid\"])\n",
    "        gamma = trial.suggest_categorical(\"gamma\", [\"scale\", \"auto\"])\n",
    "\n",
    "        base_estimator = SVC(\n",
    "            C=C,\n",
    "            kernel=kernel,\n",
    "            gamma=gamma\n",
    "        )\n",
    "\n",
    "    adaboost = AdaBoostClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=learning_rate,\n",
    "        algorithm=algorithm,\n",
    "        estimator=base_estimator\n",
    "    )\n",
    "\n",
    "    adaboost.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = adaboost.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    return accuracy\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "best_params = study.best_params\n",
    "best_accuracy = study.best_value\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f23983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_accuracy = study.best_value\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best accuracy:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fa7a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_best = AdaBoostClassifier(\n",
    "    n_estimators=best_params['n_estimators'], # 147\n",
    "    learning_rate=best_params['learning_rate'], # 0.42329791924956006\n",
    "    algorithm=best_params['algorithm'], # SAMME\n",
    "    estimator=DecisionTreeClassifier(criterion = best_params['criterion'], # gini\n",
    "                                     max_depth = best_params['max_depth'], # 27\n",
    "                                     min_samples_split = best_params['min_samples_split']) # 7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc640f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adaboost_best = adaboost_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ffd875",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = adaboost_best.predict(X_test)\n",
    "\n",
    "print(\"Accuracy = \",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision = \",precision_score(y_test,y_pred))\n",
    "print(\"Recall = \",recall_score(y_test,y_pred))\n",
    "print(\"F1 Score\", f1_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b2204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(y_test,y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Churn Confusion Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16febc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(scaler,open(\"scaler_raw.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41ffac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(adaboost_best,open(\"adaboost_best_raw.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d1a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
